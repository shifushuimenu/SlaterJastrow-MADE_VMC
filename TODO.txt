[ ] symmetry projection 

[ ] in kinetic_term(): 
    M[...] = 2**i + 2**j => too large ints cannot be represented by C-long 

[ ] direct sampling from a Pfaffian ansatz 

[ ] speed up bitcoding using bitarray.py  

[ ] stochastic reconfiguration of LBFGS instead of stochastic gradient descent optimization 

[ ] argparse 

[ ] When computing the local energy, the ratio of amplitudes for two very similar states is required,
    which differ only by one electron position. Still, both configurations must be passed through 
    the neural network. This is the 4fficiency bottleneck. Is it possible to evaluate only part of the 
    neural network ? Is it possible to take the gradient wth respect to the inputs and then calculate 
    the change in the probability amplitude given the small change in the input (just one position) ?

    Problem: (1) Calculate log_prob_MADE(config) in a single pass
             (2) Calculate log_prob_Slater(config)
             The sum of (1) and (2) is not correctly normalized. For a correct normalization all conditional 
             probabilities coming from the Slater determinant are needed. 
             How can the fermionic conditional probabilities of a similar state be reused ?
    => Low-rank update of the conditional probabilities given those of a similar state.  

[ ] verify speedup of advanced update of slater sampler  

[ ] How can a Pfaffian be expanded in terms of Slater determinants ?

[ ] backflow transformation

[ ] Use an ansatz with complex numbers 

[ ] Use the Thouless theorem to find a non-redundant parametrization of any Slater determinant. 
    There are 2*Np*(Ns-Np) parameters, or Np*(Ns-Np) complex parameters, rather than Ns*Ns parameters.

 
