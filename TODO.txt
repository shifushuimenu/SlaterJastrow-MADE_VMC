[ ] Hartree-Fock energy must be variational. 
    Something with the calculation of Hartree-Fock ground state energy is wrong. 
    The HF eigenstates seem to be correct, though. 

[ ] How can convergence towards an excited state be prevented ? Is this a problem at all ? 

[ ] symmetry projection 

[ ] Use tensorboard to visualize the learning progess. 

[x] in kinetic_term(): 
[x]    M[...] = 2**i + 2**j => too large ints cannot be represented by C-long 

[x] direct sampling from a Pfaffian ansatz 

[ ] speed up bitcoding using bitarray.py  

[ ] Better optimization techniques:
    - stochastic reconfiguration or LBFGS instead of stochastic gradient descent optimization
    - Obtain the spectrum of the Fisher information matrix to characterize the learning manifold.  
    - AMSgrad 
    - Linear Method 

[ ] argparse 

[x] When computing the local energy, the ratio of amplitudes for two very similar states is required,
    which differ only by one electron position. Still, both configurations must be passed through 
    the neural network. This is the 4fficiency bottleneck. Is it possible to evaluate only part of the 
    neural network ? Is it possible to take the gradient wth respect to the inputs and then calculate 
    the change in the probability amplitude given the small change in the input (just one position) ?

    Problem: (1) Calculate log_prob_MADE(config) in a single pass
             (2) Calculate log_prob_Slater(config)
             The sum of (1) and (2) is not correctly normalized. For a correct normalization all conditional 
             probabilities coming from the Slater determinant are needed. 
             How can the fermionic conditional probabilities of a similar state be reused ?
     => Low-rank update of the conditional probabilities given those of a similar state.  (seems very complicated)
    Solution: While sampling a configuration, compute the probabilities of all states reachable by the 
              kinetic operator along the way. These are O(Nsites) states. (Note that the hopping matrix element could 
              be bond-dependent.) Output: (sample, sample_prob, local_kinetic_energy) 

[x] verify speedup of advanced update of slater sampler  

[x] test joint optimization of Slater determinant and Jastrow factor => unstable with SGD => try stochastic reconfiguration

[ ] introduce a scheduler for the learning rates of MADE and of the P-matrix 

[x] How can a Pfaffian be expanded in terms of Slater determinants ?

[ ] backflow transformation

[ ] Add network for correcting the sign structure. Use a residual neural network as in Stokes at el. (2020). Since configurations are sampled 
    according to the square of the wavefunction amplitude, the sign structure does not affect the sampling, but only 
    the evaluation of the local energy. 

[ ] Can the autoregressive property be combined with the convolutional structure of the neural network ? 

[ ] Use an ansatz with complex numbers 

[ ] Time evolution via time-dependent variational principle. 

[ ] Use the Thouless theorem to find a non-redundant parametrization of any Slater determinant. 
    There are 2*Np*(Ns-Np) parameters, or Np*(Ns-Np) complex parameters, rather than Ns*Ns parameters.

[ ] Exact diagonalization and Slater-Jastrow ansatz give the same result as long as the Hartree-Fock 
    ansatz used as the Slater determinant is for a closed-shell system. How to deal with degeneracies
    in the Hartree-Fock spectrum ?  

[ ] Singular matrices in lowrank_kinetic(). What is the root cause ? Is there a connection with degeneracies in the Hartree-Fock spectrum ? 
    Could the problem be solved by adding invertibility noise ?  

[ ] Check division by zero in slater_sampler_ordered.py:535 (549, 691)

[ ] Test calculation of kinetic energy (separate file) and benchmark the speedup. Make a figure for the publication.


# UNCLEAR THINGS
[ ] Where is `bias_zeroth_component` acting ?
